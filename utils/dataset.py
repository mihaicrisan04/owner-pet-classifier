import torch
from torch.utils.data import Dataset, DataLoader
import os
import cv2
import xml.etree.ElementTree as ET
from torchvision import transforms
from pathlib import Path

class ObjectDetectionDataset(Dataset):
    """
    A PyTorch Dataset for loading images and their bounding box annotations
    in Pascal VOC format (generated by LabelImg).
    """
    def __init__(self, data_dir, transform=None):
        """
        Args:
            data_dir (string): Directory with all the images and XML annotations.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]
        self.annotations = {} # Store annotations by image filename

        # Load all annotations
        for img_file in self.image_files:
            base_name = os.path.splitext(img_file)[0]
            xml_path = os.path.join(data_dir, base_name + '.xml')
            if os.path.exists(xml_path):
                self.annotations[img_file] = self._parse_pascal_voc(xml_path)
            else:
                # Handle cases where an image might not have an annotation (less common for training data)
                self.annotations[img_file] = []

    def _parse_pascal_voc(self, xml_path):
        """Parses a Pascal VOC XML file and returns a list of bounding boxes and labels."""
        tree = ET.parse(xml_path)
        root = tree.getroot()
        boxes = []
        labels = []
        for obj in root.findall('object'):
            label = obj.find('name').text
            bbox = obj.find('bndbox')
            xmin = int(float(bbox.find('xmin').text))
            ymin = int(float(bbox.find('ymin').text))
            xmax = int(float(bbox.find('xmax').text))
            ymax = int(float(bbox.find('ymax').text))
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(label)
        return {'boxes': torch.tensor(boxes, dtype=torch.float32),
                'labels': labels} # Keep labels as strings for now

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = self.image_files[idx]
        img_path = os.path.join(self.data_dir, img_name)
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB
        image = torch.from_numpy(image).permute(2, 0, 1).float() # Convert to PyTorch tensor (C, H, W)

        annotation = self.annotations.get(img_name, {'boxes': torch.tensor([], dtype=torch.float32), 'labels': []})

        # You might want to crop the image to the bounding box here if you're training
        # a classifier on the cropped region. For detection, you'd return the image and targets.
        # For this project (classifying cropped regions), let's add cropping.

        # Assuming there's only one object per image for simplicity in this initial dataset loader
        # (which should be true for your enrollment data). Adapt if you have multiple objects per image.
        cropped_image = image # Default to the whole image if no annotation or multiple annotations
        target_label = 'background' # Default label

        if annotation['boxes'].numel() > 0:
             # Take the first bounding box for simplicity
            box = annotation['boxes'][0]
            xmin, ymin, xmax, ymax = [int(b.item()) for b in box] # Convert to int
            # Ensure coordinates are within image bounds
            h, w = image.shape[1:] # H, W
            xmin = max(0, xmin)
            ymin = max(0, ymin)
            xmax = min(w, xmax)
            ymax = min(h, ymax)

            # Crop the image using slicing
            cropped_image = image[:, ymin:ymax, xmin:xmax]
            target_label = annotation['labels'][0]


        sample = {'image': cropped_image, 'label': target_label}

        if self.transform:
            sample['image'] = self.transform(sample['image'])
            # Transformations might also need to apply to bounding boxes if not cropping

        return sample

# Example Usage (for testing the dataset loader)
if __name__ == '__main__':
    # Define a simple transformation
    transform = transforms.Compose([
        transforms.Resize((128, 128)), # Resize cropped images
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization
    ])

# utils/dataset.py
import torch
from torch.utils.data import Dataset, DataLoader
import os
import cv2
import xml.etree.ElementTree as ET
from torchvision import transforms

class ObjectDetectionDataset(Dataset):
    """
    A PyTorch Dataset for loading images and their bounding box annotations
    in Pascal VOC format (generated by LabelImg).
    """
    def __init__(self, data_dir, transform=None):
        """
        Args:
            data_dir (string): Directory with all the images and XML annotations.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]
        self.annotations = {} # Store annotations by image filename

        # Load all annotations
        for img_file in self.image_files:
            base_name = os.path.splitext(img_file)[0]
            xml_path = os.path.join(data_dir, base_name + '.xml')
            if os.path.exists(xml_path):
                self.annotations[img_file] = self._parse_pascal_voc(xml_path)
            else:
                # Handle cases where an image might not have an annotation (less common for training data)
                self.annotations[img_file] = []

    def _parse_pascal_voc(self, xml_path):
        """Parses a Pascal VOC XML file and returns a list of bounding boxes and labels."""
        tree = ET.parse(xml_path)
        root = tree.getroot()
        boxes = []
        labels = []
        for obj in root.findall('object'):
            label = obj.find('name').text
            bbox = obj.find('bndbox')
            xmin = int(float(bbox.find('xmin').text))
            ymin = int(float(bbox.find('ymin').text))
            xmax = int(float(bbox.find('xmax').text))
            ymax = int(float(bbox.find('ymax').text))
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(label)
        return {'boxes': torch.tensor(boxes, dtype=torch.float32),
                'labels': labels} # Keep labels as strings for now

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = self.image_files[idx]
        img_path = os.path.join(self.data_dir, img_name)
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB
        image = torch.from_numpy(image).permute(2, 0, 1).float() # Convert to PyTorch tensor (C, H, W)

        annotation = self.annotations.get(img_name, {'boxes': torch.tensor([], dtype=torch.float32), 'labels': []})

        # You might want to crop the image to the bounding box here if you're training
        # a classifier on the cropped region. For detection, you'd return the image and targets.
        # For this project (classifying cropped regions), let's add cropping.

        # Assuming there's only one object per image for simplicity in this initial dataset loader
        # (which should be true for your enrollment data). Adapt if you have multiple objects per image.
        cropped_image = image # Default to the whole image if no annotation or multiple annotations
        target_label = 'background' # Default label

        if annotation['boxes'].numel() > 0:
             # Take the first bounding box for simplicity
            box = annotation['boxes'][0]
            xmin, ymin, xmax, ymax = [int(b.item()) for b in box] # Convert to int
            # Ensure coordinates are within image bounds
            h, w = image.shape[1:] # H, W
            xmin = max(0, xmin)
            ymin = max(0, ymin)
            xmax = min(w, xmax)
            ymax = min(h, ymax)

            # Crop the image using slicing
            cropped_image = image[:, ymin:ymax, xmin:xmax]
            target_label = annotation['labels'][0]


        sample = {'image': cropped_image, 'label': target_label}

        if self.transform:
            sample['image'] = self.transform(sample['image'])
            # Transformations might also need to apply to bounding boxes if not cropping

        return sample

# Example Usage (for testing the dataset loader)
if __name__ == '__main__':
    ROOT_DIR = Path(__file__).parent.parent.absolute()
    DATA_DIR = os.path.join(ROOT_DIR, 'data')

    # Define a simple transformation
    transform = transforms.Compose([
        transforms.Resize((128, 128)), # Resize cropped images
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization
    ])

    # Load the training data for owner
    owner_train_dataset = ObjectDetectionDataset(os.path.join(DATA_DIR, 'owner'), transform=transform)
    owner_train_dataloader = DataLoader(owner_train_dataset, batch_size=4, shuffle=True)

    print(f"Loaded {len(owner_train_dataset)} owner training images.")

    # Iterate through a batch
    for i, sample in enumerate(owner_train_dataloader):
        images = sample['image']
        labels = sample['label']
        print(f"Batch {i+1}: Image batch shape: {images.shape}, Labels: {labels}")

        # You can visualize images here if needed for debugging
        # import matplotlib.pyplot as plt
        # plt.imshow(images[0].permute(1, 2, 0).numpy().astype('uint8')) # Convert back to HWC and uint8 for display
        # plt.title(f"Label: {labels[0]}")
        # plt.show()

        if i == 0: # Just show the first batch
            break

    # Load the training data for pet
    pet_train_dataset = ObjectDetectionDataset(os.path.join(DATA_DIR, 'pet'), transform=transform)
    pet_train_dataloader = DataLoader(pet_train_dataset, batch_size=4, shuffle=True)

    print(f"Loaded {len(pet_train_dataset)} pet training images.")
    for i, sample in enumerate(pet_train_dataloader):
         images = sample['image']
         labels = sample['label']
         print(f"Batch {i+1}: Image batch shape: {images.shape}, Labels: {labels}")
         if i == 0:
             break

    # Load the training data for another person (if used)
    # if os.path.exists(os.path.join('data', 'another_person_enrollment')) and len(os.listdir(os.path.join('data', 'another_person_enrollment'))) > 0:
    #     another_person_train_dataset = ObjectDetectionDataset(os.path.join('data', 'another_person_enrollment'), transform=transform)
    #     another_person_train_dataloader = DataLoader(another_person_train_dataset, batch_size=4, shuffle=True)
    #     print(f"Loaded {len(another_person_train_dataset)} another person training images.")
    #     for i, sample in enumerate(another_person_train_dataloader):
    #         images = sample['image']
    #         labels = sample['label']
    #         print(f"Batch {i+1}: Image batch shape: {images.shape}, Labels: {labels}")
    #         if i == 0:
    #             break

